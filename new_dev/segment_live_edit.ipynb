{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1f42b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(checkpoints_dir='checkpoints', results_dir='results', geom_name='feats2Geom', batchSize=1, dataroot='examples/test', depthroot='', input_nc=3, output_nc=1, geom_nc=3, every_feat=1, num_classes=55, midas=0, ngf=64, n_blocks=3, size=1080, cuda=False, n_cpu=8, which_epoch='latest', aspect_ratio=1.0, mode='test', load_size=1080, crop_size=1080, max_dataset_size=inf, preprocess='resize_and_crop', no_flip=False, norm='instance', predict_depth=0, save_input=0, reconstruct=0, how_many=100, name='opensketch_style')\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"OPENCV_VIDEOIO_PRIORITY_MSMF\"] = \"0\"\n",
    "import IPython.display\n",
    "import time\n",
    "from IPython.core.display import HTML\n",
    "import io\n",
    "import argparse\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "from typing import Tuple, Union\n",
    "import math\n",
    "\n",
    "\n",
    "norm_layer = nn.InstanceNorm2d\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        norm_layer(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        norm_layer(in_features)\n",
    "                        ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9, sigmoid=True):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model0 = [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    norm_layer(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "        self.model0 = nn.Sequential(*model0)\n",
    "\n",
    "        # Downsampling\n",
    "        model1 = []\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model1 += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        norm_layer(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "\n",
    "        model2 = []\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model2 += [ResidualBlock(in_features)]\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "\n",
    "        # Upsampling\n",
    "        model3 = []\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model3 += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        norm_layer(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "\n",
    "        # Output layer\n",
    "        model4 = [  nn.ReflectionPad2d(3),\n",
    "                        nn.Conv2d(64, output_nc, 7)]\n",
    "        if sigmoid:\n",
    "            model4 += [nn.Sigmoid()]\n",
    "\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        out = self.model0(x)\n",
    "        out = self.model1(out)\n",
    "        out = self.model2(out)\n",
    "        out = self.model3(out)\n",
    "        out = self.model4(out)\n",
    "\n",
    "        return out   \n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir, stop=10000):\n",
    "    images = []\n",
    "    count = 0\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "                count += 1\n",
    "            if count >= stop:\n",
    "                return images\n",
    "    return images\n",
    "\n",
    "class UnpairedDepthDataset(data.Dataset):\n",
    "    def __init__(self, root, root2, opt, transforms_r=None, mode='train', midas=False, depthroot=''):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.midas = midas\n",
    "        all_img = make_dataset(self.root)\n",
    "        self.depth_maps = 0\n",
    "        self.data = all_img\n",
    "        self.mode = mode\n",
    "        self.transform_r = transforms.Compose(transforms_r)\n",
    "        self.opt = opt\n",
    "        self.min_length = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data[index]\n",
    "        basename = os.path.basename(img_path)\n",
    "        base = basename.split('.')[0]\n",
    "        img_r = Image.open(img_path).convert('RGB')\n",
    "        transform_params = get_params(self.opt, img_r.size)\n",
    "        A_transform = get_transform(self.opt, transform_params, grayscale=(self.opt.input_nc == 1), norm=False)   \n",
    "        A_transform = self.transform_r\n",
    "        img_r = A_transform(img_r )\n",
    "        img_depth = 0\n",
    "        label = 0\n",
    "        input_dict = {'r': img_r, 'depth': img_depth, 'path': img_path, 'index': index, 'name' : base, 'label': label}\n",
    "        return input_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.min_length\n",
    "\n",
    "def __make_power_2(img, base, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    h = int(round(oh / base) * base)\n",
    "    w = int(round(ow / base) * base)\n",
    "    if (h == oh) and (w == ow):\n",
    "        return img\n",
    "\n",
    "    __print_size_warning(ow, oh, w, h)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    if (ow == target_width):\n",
    "        return img\n",
    "    w = target_width\n",
    "    h = int(target_width * oh / ow)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __crop(img, pos, size):\n",
    "    ow, oh = img.size\n",
    "    x1, y1 = pos\n",
    "    tw = th = size\n",
    "    color = (255, 255, 255)\n",
    "    if img.mode == 'L':\n",
    "        color = (255)\n",
    "    elif img.mode == 'RGBA':\n",
    "        color = (255, 255, 255, 255)\n",
    "\n",
    "    if (ow > tw and oh > th):\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "    elif ow > tw:\n",
    "        ww = img.crop((x1, 0, x1 + tw, oh))\n",
    "        return add_margin(ww, size, 0, (th-oh)//2, color)\n",
    "    elif oh > th:\n",
    "        hh = img.crop((0, y1, ow, y1 + th))\n",
    "        return add_margin(hh, size, (tw-ow)//2, 0, color)\n",
    "    return img\n",
    "\n",
    "def add_margin(pil_img, newsize, left, top, color=(255, 255, 255)):\n",
    "    width, height = pil_img.size\n",
    "    result = Image.new(pil_img.mode, (newsize, newsize), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "def __flip(img, flip):\n",
    "    if flip:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n",
    "\n",
    "def __print_size_warning(ow, oh, w, h):\n",
    "    \"\"\"Print warning information about image size(only print once)\"\"\"\n",
    "    if not hasattr(__print_size_warning, 'has_printed'):\n",
    "        print(\"The image size needs to be a multiple of 4. \"\n",
    "              \"The loaded image size was (%d, %d), so it was adjusted to \"\n",
    "              \"(%d, %d). This adjustment will be done to all images \"\n",
    "              \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n",
    "        __print_size_warning.has_printed = True\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt.preprocess == 'resize_and_crop':\n",
    "        new_h = new_w = opt.load_size\n",
    "    elif opt.preprocess == 'scale_width_and_crop':\n",
    "        new_w = opt.load_size\n",
    "        new_h = opt.load_size * h // w\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n",
    "    flip = random.random() > 0.5\n",
    "    return {'crop_pos': (x, y), 'flip': flip}\n",
    "\n",
    "def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True, norm=True):\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "    if 'resize' in opt.preprocess:\n",
    "        osize = [opt.load_size, opt.load_size]\n",
    "        transform_list.append(transforms.Resize(osize, method))\n",
    "    elif 'scale_width' in opt.preprocess:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
    "\n",
    "    if 'crop' in opt.preprocess:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomCrop(opt.crop_size))\n",
    "        else:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
    "\n",
    "    if opt.preprocess == 'none':\n",
    "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    if not opt.no_flip:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        elif params['flip']:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if not grayscale:\n",
    "            if norm:\n",
    "                transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--name', required=True, type=str, help='name of this experiment')\n",
    "# parser.add_argument('--checkpoints_dir', type=str, default='checkpoints', help='Where the model checkpoints are saved')\n",
    "# parser.add_argument('--results_dir', type=str, default='results', help='where to save result images')\n",
    "# parser.add_argument('--geom_name', type=str, default='feats2Geom', help='name of the geometry predictor')\n",
    "# parser.add_argument('--batchSize', type=int, default=1, help='size of the batches')\n",
    "# parser.add_argument('--dataroot', type=str, default='', help='root directory of the dataset')\n",
    "# parser.add_argument('--depthroot', type=str, default='', help='dataset of corresponding ground truth depth maps')\n",
    "# parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')\n",
    "# parser.add_argument('--output_nc', type=int, default=1, help='number of channels of output data')\n",
    "# parser.add_argument('--geom_nc', type=int, default=3, help='number of channels of geometry data')\n",
    "# parser.add_argument('--every_feat', type=int, default=1, help='use transfer features for the geometry loss')\n",
    "# parser.add_argument('--num_classes', type=int, default=55, help='number of classes for inception')\n",
    "# parser.add_argument('--midas', type=int, default=0, help='use midas depth map')\n",
    "# parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
    "# parser.add_argument('--n_blocks', type=int, default=3, help='number of resnet blocks for generator')\n",
    "# parser.add_argument('--size', type=int, default=256, help='size of the data (squared assumed)')\n",
    "# parser.add_argument('--cuda', action='store_true', help='use GPU computation', default=True)\n",
    "# parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "# parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load from')\n",
    "# parser.add_argument('--aspect_ratio', type=float, default=1.0, help='The ratio width/height. The final height of the load image will be crop_size/aspect_ratio')\n",
    "# parser.add_argument('--mode', type=str, default='test', help='train, val, test, etc')\n",
    "# parser.add_argument('--load_size', type=int, default=256, help='scale images to this size')\n",
    "# parser.add_argument('--crop_size', type=int, default=256, help='then crop to this size')\n",
    "# parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "# parser.add_argument('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n",
    "# parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "# parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization')\n",
    "# parser.add_argument('--predict_depth', type=int, default=0, help='run geometry prediction on the generated images')\n",
    "# parser.add_argument('--save_input', type=int, default=0, help='save input image')\n",
    "# parser.add_argument('--reconstruct', type=int, default=0, help='get reconstruction')\n",
    "# parser.add_argument('--how_many', type=int, default=100, help='number of images to test')\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "opt = {\n",
    "    'checkpoints_dir': 'checkpoints',\n",
    "    'results_dir': 'results',\n",
    "    'geom_name': 'feats2Geom',\n",
    "    'batchSize': 1,\n",
    "    'dataroot': 'examples/test',\n",
    "    'depthroot': '',\n",
    "    'input_nc': 3,\n",
    "    'output_nc': 1,\n",
    "    'geom_nc': 3,\n",
    "    'every_feat': 1,\n",
    "    'num_classes': 55,\n",
    "    'midas': 0,\n",
    "    'ngf': 64,\n",
    "    'n_blocks': 3,\n",
    "    'size': 1080, #256,\n",
    "    'cuda': True,\n",
    "    'n_cpu': 8,\n",
    "    'which_epoch': 'latest',\n",
    "    'aspect_ratio': 1.0,\n",
    "    'mode': 'test',\n",
    "    'load_size': 1080, #256,\n",
    "    'crop_size': 1080, #256,\n",
    "    'max_dataset_size': float(\"inf\"),\n",
    "    'preprocess': 'resize_and_crop',\n",
    "    'no_flip': False,  # Default is False because it's a store_true argument\n",
    "    'norm': 'instance',\n",
    "    'predict_depth': 0,\n",
    "    'save_input': 0,\n",
    "    'reconstruct': 0,\n",
    "    'how_many': 100,\n",
    "}\n",
    "\n",
    "opt['name'] = 'opensketch_style'\n",
    "opt['cuda'] = False\n",
    "opt = argparse.Namespace(**opt)\n",
    "print(opt)\n",
    "\n",
    "# opt.no_flip = True\n",
    "# Check for CUDA availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and opt.cuda else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "\n",
    "def visualize(image,detection_result) -> np.ndarray:\n",
    "    \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "    Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "    Returns:\n",
    "    Image with bounding boxes.\n",
    "    \"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw keypoints\n",
    "        for keypoint in detection.keypoints:\n",
    "            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
    "                                                         width, height)\n",
    "            color, thickness, radius = (0, 255, 0), 2, 2\n",
    "            cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "    # Draw label and score\n",
    "            category = detection.categories[0]\n",
    "            category_name = category.category_name\n",
    "            category_name = '' if category_name is None else category_name\n",
    "            probability = round(category.score, 2)\n",
    "            result_text = category_name + ' (' + str(probability) + ')'\n",
    "            text_location = (MARGIN + bbox.origin_x,\n",
    "                             MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "            cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                        FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def crop_it(image, detection_result):\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    if bbox.origin_x-50>0:\n",
    "        init_x_adjust = -50\n",
    "    else:\n",
    "        init_x_adjust = 0\n",
    "    if bbox.origin_x + bbox.width + 50 < width:\n",
    "        init_w_adjust = 50\n",
    "    else:\n",
    "        init_w_adjust = 0\n",
    "    if bbox.origin_y-100>0:\n",
    "        init_y_adjust = -100\n",
    "    else:\n",
    "        init_y_adjust = 0\n",
    "    if bbox.origin_y + bbox.height + 50 < height:\n",
    "        init_h_adjust = 50\n",
    "    else:\n",
    "        init_h_adjust = 0    \n",
    "    return annotated_image[bbox.origin_y+init_y_adjust:bbox.origin_y+bbox.height+init_h_adjust, bbox.origin_x+init_x_adjust:bbox.origin_x + bbox.width+init_w_adjust]\n",
    "\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "    \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "    if not (is_valid_normalized_value(normalized_x) and\n",
    "        is_valid_normalized_value(normalized_y)):\n",
    "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
    "        return None\n",
    "    \n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a89e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44066c01-a494-4c31-950a-51f7c83af26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BG_COLOR = (192, 192, 192) # gray\n",
    "MASK_COLOR = (255, 255, 255) # white\n",
    "\n",
    "# Create the options that will be used for ImageSegmenter\n",
    "base_options = python.BaseOptions(model_asset_path='deeplab_v3.tflite')\n",
    "options = vision.ImageSegmenterOptions(base_options=base_options,output_category_mask=True)\n",
    "switch_it = 0\n",
    "with torch.no_grad():\n",
    "    # Networks\n",
    "    net_G = Generator(opt.input_nc, opt.output_nc, opt.n_blocks).to(device)\n",
    "    # Load state dicts\n",
    "    net_G.load_state_dict(torch.load(os.path.join(opt.checkpoints_dir, opt.name, f'netG_A_{opt.which_epoch}.pth'), map_location=device, weights_only=True))\n",
    "    # print('loaded', os.path.join(opt.checkpoints_dir, opt.name, f'netG_A_{opt.which_epoch}.pth'))\n",
    "    # Set model's test mode\n",
    "    net_G.eval()\n",
    "    transforms_r = [transforms.Resize(int(opt.size), Image.BICUBIC),\n",
    "                    transforms.ToTensor()]\n",
    "    the_transformation = transforms.Compose(transforms_r)\n",
    "    with vision.ImageSegmenter.create_from_options(options) as segmenter:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "        running = 0\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            cv2.startWindowThread()\n",
    "            try:\n",
    "                if ret == True:\n",
    "                    image = frame.copy()\n",
    "                    # half = image.copy()\n",
    "                    half = cv2.resize(image, (1920, 1080))\n",
    "                    frame_rgb = cv2.cvtColor(half, cv2.COLOR_BGR2RGB)\n",
    "                    pil_image = Image.fromarray(frame_rgb)\n",
    "                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.asarray(pil_image))\n",
    "\n",
    "                    segmentation_result = segmenter.segment(mp_image)\n",
    "                    category_mask = segmentation_result.category_mask\n",
    "                    frame_rgb = cv2.cvtColor(half, cv2.COLOR_BGR2RGB)\n",
    "                    pil_image = Image.fromarray(frame_rgb)\n",
    "                    img_r = the_transformation(pil_image)\n",
    "                    input_dict = {'r': img_r}\n",
    "                    img_r2 = Variable(input_dict['r']).to(device)\n",
    "                    real_A = img_r2\n",
    "                    new_image = net_G(real_A)\n",
    "                    image_np = new_image.cpu().detach().numpy()  # Move to CPU and convert to NumPy\n",
    "                    \n",
    "                    # # Transpose from (C, H, W) to (H, W, C)\n",
    "                    image_np = np.transpose(image_np, (1, 2, 0))\n",
    "                    \n",
    "                    # # Scale from [0, 1] to [0, 255] and convert to uint8\n",
    "                    image_np = (image_np * 255).astype(np.uint8)\n",
    "                    # Generate solid color images for showing the output segmentation mask.\n",
    "                    image_data = mp_image.numpy_view()\n",
    "                    fg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
    "                    bg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
    "                    if switch_it%2==0:\n",
    "                        fg_image[:] = image_np\n",
    "                        bg_image[:] = half\n",
    "                    else:\n",
    "                        fg_image[:] = half\n",
    "                        bg_image[:] = image_np\n",
    "                    \n",
    "                    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2\n",
    "                    output_image = np.where(condition, fg_image, bg_image)\n",
    "                    cv2.imshow('segmented', output_image)\n",
    "                    cv2.imshow('true', half)\n",
    "                    pressedKey = cv2.waitKey(1) & 0xFF\n",
    "                    if pressedKey == ord('q'):\n",
    "                        break\n",
    "                    elif pressedKey == ord('w'):\n",
    "                        switch_it+=1\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "        for i in range(5):\n",
    "            cv2.waitKey(1)\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11511d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe770d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e385b-2eb8-4fbd-822b-9ebb0c18256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a58247-79c8-41e5-8b7f-384c511d22e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
