{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f01eef-b1bb-41a8-9008-ee32a9fdb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(checkpoints_dir='checkpoints', results_dir='results', geom_name='feats2Geom', batchSize=1, dataroot='examples/test', depthroot='', input_nc=3, output_nc=1, geom_nc=3, every_feat=1, num_classes=55, midas=0, ngf=64, n_blocks=3, size=256, cuda=False, n_cpu=8, which_epoch='latest', aspect_ratio=1.0, mode='test', load_size=256, crop_size=256, max_dataset_size=inf, preprocess='resize_and_crop', no_flip=False, norm='instance', predict_depth=0, save_input=0, reconstruct=0, how_many=100, name='anime_style')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"OPENCV_VIDEOIO_PRIORITY_MSMF\"] = \"0\"\n",
    "import IPython.display\n",
    "import time\n",
    "from IPython.core.display import HTML\n",
    "import io\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "norm_layer = nn.InstanceNorm2d\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        norm_layer(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        norm_layer(in_features)\n",
    "                        ]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9, sigmoid=True):\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial convolution block\n",
    "        model0 = [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    norm_layer(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "        self.model0 = nn.Sequential(*model0)\n",
    "        # Downsampling\n",
    "        model1 = []\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model1 += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        norm_layer(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        model2 = []\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model2 += [ResidualBlock(in_features)]\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        # Upsampling\n",
    "        model3 = []\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model3 += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        norm_layer(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        # Output layer\n",
    "        model4 = [  nn.ReflectionPad2d(3),\n",
    "                        nn.Conv2d(64, output_nc, 7)]\n",
    "        if sigmoid:\n",
    "            model4 += [nn.Sigmoid()]\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        out = self.model0(x)\n",
    "        out = self.model1(out)\n",
    "        out = self.model2(out)\n",
    "        out = self.model3(out)\n",
    "        out = self.model4(out)\n",
    "        return out   \n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir, stop=10000):\n",
    "    images = []\n",
    "    count = 0\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "                count += 1\n",
    "            if count >= stop:\n",
    "                return images\n",
    "    return images\n",
    "\n",
    "class UnpairedDepthDataset(data.Dataset):\n",
    "    def __init__(self, root, root2, opt, transforms_r=None, mode='train', midas=False, depthroot=''):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.midas = midas\n",
    "        all_img = make_dataset(self.root)\n",
    "        self.depth_maps = 0\n",
    "        self.data = all_img\n",
    "        self.mode = mode\n",
    "        self.transform_r = transforms.Compose(transforms_r)\n",
    "        self.opt = opt\n",
    "        self.min_length = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data[index]\n",
    "        basename = os.path.basename(img_path)\n",
    "        base = basename.split('.')[0]\n",
    "        img_r = Image.open(img_path).convert('RGB')\n",
    "        transform_params = get_params(self.opt, img_r.size)\n",
    "        A_transform = get_transform(self.opt, transform_params, grayscale=(self.opt.input_nc == 1), norm=False)   \n",
    "        A_transform = self.transform_r\n",
    "        img_r = A_transform(img_r )\n",
    "        img_depth = 0\n",
    "        label = 0\n",
    "        input_dict = {'r': img_r, 'depth': img_depth, 'path': img_path, 'index': index, 'name' : base, 'label': label}\n",
    "        return input_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.min_length\n",
    "\n",
    "def __make_power_2(img, base, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    h = int(round(oh / base) * base)\n",
    "    w = int(round(ow / base) * base)\n",
    "    if (h == oh) and (w == ow):\n",
    "        return img\n",
    "\n",
    "    __print_size_warning(ow, oh, w, h)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    if (ow == target_width):\n",
    "        return img\n",
    "    w = target_width\n",
    "    h = int(target_width * oh / ow)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __crop(img, pos, size):\n",
    "    ow, oh = img.size\n",
    "    x1, y1 = pos\n",
    "    tw = th = size\n",
    "    color = (255, 255, 255)\n",
    "    if img.mode == 'L':\n",
    "        color = (255)\n",
    "    elif img.mode == 'RGBA':\n",
    "        color = (255, 255, 255, 255)\n",
    "\n",
    "    if (ow > tw and oh > th):\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "    elif ow > tw:\n",
    "        ww = img.crop((x1, 0, x1 + tw, oh))\n",
    "        return add_margin(ww, size, 0, (th-oh)//2, color)\n",
    "    elif oh > th:\n",
    "        hh = img.crop((0, y1, ow, y1 + th))\n",
    "        return add_margin(hh, size, (tw-ow)//2, 0, color)\n",
    "    return img\n",
    "\n",
    "def add_margin(pil_img, newsize, left, top, color=(255, 255, 255)):\n",
    "    width, height = pil_img.size\n",
    "    result = Image.new(pil_img.mode, (newsize, newsize), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "def __flip(img, flip):\n",
    "    if flip:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n",
    "\n",
    "def __print_size_warning(ow, oh, w, h):\n",
    "    \"\"\"Print warning information about image size(only print once)\"\"\"\n",
    "    if not hasattr(__print_size_warning, 'has_printed'):\n",
    "        print(\"The image size needs to be a multiple of 4. \"\n",
    "              \"The loaded image size was (%d, %d), so it was adjusted to \"\n",
    "              \"(%d, %d). This adjustment will be done to all images \"\n",
    "              \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n",
    "        __print_size_warning.has_printed = True\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt.preprocess == 'resize_and_crop':\n",
    "        new_h = new_w = opt.load_size\n",
    "    elif opt.preprocess == 'scale_width_and_crop':\n",
    "        new_w = opt.load_size\n",
    "        new_h = opt.load_size * h // w\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n",
    "    flip = random.random() > 0.5\n",
    "    return {'crop_pos': (x, y), 'flip': flip}\n",
    "\n",
    "def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True, norm=True):\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "    if 'resize' in opt.preprocess:\n",
    "        osize = [opt.load_size, opt.load_size]\n",
    "        transform_list.append(transforms.Resize(osize, method))\n",
    "    elif 'scale_width' in opt.preprocess:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
    "\n",
    "    if 'crop' in opt.preprocess:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomCrop(opt.crop_size))\n",
    "        else:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
    "\n",
    "    if opt.preprocess == 'none':\n",
    "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    if not opt.no_flip:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        elif params['flip']:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if not grayscale:\n",
    "            if norm:\n",
    "                transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "opt = {\n",
    "    'checkpoints_dir': 'checkpoints',\n",
    "    'results_dir': 'results',\n",
    "    'geom_name': 'feats2Geom',\n",
    "    'batchSize': 1,\n",
    "    'dataroot': 'examples/test',\n",
    "    'depthroot': '',\n",
    "    'input_nc': 3,\n",
    "    'output_nc': 1,\n",
    "    'geom_nc': 3,\n",
    "    'every_feat': 1,\n",
    "    'num_classes': 55,\n",
    "    'midas': 0,\n",
    "    'ngf': 64,\n",
    "    'n_blocks': 3,\n",
    "    'size': 256, #1080\n",
    "    'cuda': True,\n",
    "    'n_cpu': 8,\n",
    "    'which_epoch': 'latest',\n",
    "    'aspect_ratio': 1.0,\n",
    "    'mode': 'test',\n",
    "    'load_size': 256, #1080\n",
    "    'crop_size': 256, #1080\n",
    "    'max_dataset_size': float(\"inf\"),\n",
    "    'preprocess': 'resize_and_crop',\n",
    "    'no_flip': False,  # Default is False because it's a store_true argument\n",
    "    'norm': 'instance',\n",
    "    'predict_depth': 0,\n",
    "    'save_input': 0,\n",
    "    'reconstruct': 0,\n",
    "    'how_many': 100,\n",
    "}\n",
    "\n",
    "# opt['name'] = 'opensketch_style'\n",
    "opt['name'] = 'anime_style'\n",
    "# opt['name'] = 'contour_style'\n",
    "opt['cuda'] = False\n",
    "opt = argparse.Namespace(**opt)\n",
    "print(opt)\n",
    "\n",
    "# opt.no_flip = True\n",
    "# Check for CUDA availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and opt.cuda else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "import onnx\n",
    "from onnx2torch import convert\n",
    "onnx_model = onnx.load(os.path.join(opt.checkpoints_dir, \"cartoonGAN\",\"cartoonGAN.onnx\" ))\n",
    "\n",
    "# Convert the ONNX model to a PyTorch model\n",
    "cartoonGAN_model = convert(onnx_model)\n",
    "\n",
    "cartoonGAN_model_path = os.path.join(opt.checkpoints_dir, \"cartoonGAN\",\"cartoonGAN.pth\" )  # Replace with the actual path to your .pth file\n",
    "state_dict = torch.load(cartoonGAN_model_path, weights_only=True)\n",
    "\n",
    "# Load the weights into the model\n",
    "cartoonGAN_model.load_state_dict(state_dict)\n",
    "\n",
    "def pre_processing(image_path, style=\"\", expand_dim=True):\n",
    "    input_image = image_path #PIL.Image.open(image_path).convert(\"RGB\")\n",
    "    input_image = np.asarray(input_image)\n",
    "    input_image = input_image.astype(np.float32)\n",
    "    input_image = input_image[:, :, [2, 1, 0]]\n",
    "    if expand_dim:\n",
    "        input_image = np.expand_dims(input_image, axis=0)\n",
    "    return torch.from_numpy(input_image)\n",
    "\n",
    "def post_processing(transformed_image, style=\"\"):\n",
    "    if not type(transformed_image) == np.ndarray:\n",
    "        transformed_image = transformed_image.numpy()\n",
    "    transformed_image = transformed_image[0]\n",
    "    transformed_image = transformed_image[:, :, [2, 1, 0]]\n",
    "    transformed_image = transformed_image * 0.5 + 0.5\n",
    "    transformed_image = transformed_image * 255\n",
    "    return transformed_image\n",
    "\n",
    "def dist_func(left, right):\n",
    "    scale = np.sqrt((right[0]-left[0])**2+(right[1]-left[1])**2)/300.0\n",
    "    if (right[0]-left[0])!=0:\n",
    "        theta = np.arctan((right[1]-left[1])/(right[0]-left[0]))+np.pi/4\n",
    "    else:\n",
    "        theta = np.pi/2\n",
    "    x = scale*np.sqrt(2)*80/np.sqrt(np.tan(theta)**2+1)\n",
    "    y = np.tan(theta)*x\n",
    "    return (x, y)\n",
    "\n",
    "def mask_picture_frame(image, corners, second):\n",
    "    \"\"\"\n",
    "    Masks the area inside the trapezoid defined by corners with pink pixels.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        corners (list of tuple): Four corner points (x, y) of the trapezoid in any order.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The modified image with the trapezoid filled with pink pixels.\n",
    "    \"\"\"\n",
    "    # Ensure corners are a numpy array\n",
    "    corners = np.array(corners, dtype=np.float32)\n",
    "\n",
    "    # Calculate the center of the corners\n",
    "    center = np.mean(corners, axis=0)\n",
    "\n",
    "    # Sort corners based on their angle from the center\n",
    "    def angle_from_center(point):\n",
    "        return np.arctan2(point[1] - center[1], point[0] - center[0])\n",
    "    \n",
    "    sorted_corners = sorted(corners, key=angle_from_center)\n",
    "\n",
    "    # Convert sorted corners back to numpy array\n",
    "    sorted_corners = np.array(sorted_corners, dtype=np.int32)\n",
    "\n",
    "    # Create a mask the same size as the image\n",
    "    mask = np.zeros_like(image, dtype=np.uint8)\n",
    "\n",
    "    # Fill the trapezoid in the mask with white (255, 255, 255)\n",
    "    cv2.fillPoly(mask, [sorted_corners], (255, 255, 255))\n",
    "\n",
    "    # Define the pink color (BGR)\n",
    "    pink_color = (255, 105, 180)  # Bright pink in BGR\n",
    "\n",
    "    # Use the mask to replace the trapezoid area with pink\n",
    "    image[np.where((mask == 255).all(axis=2))] = second[np.where((mask == 255).all(axis=2))]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73da8a1-0189-4e85-88a7-ab9b322ed8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    switch_it=0\n",
    "    # Networks\n",
    "    net_G = Generator(opt.input_nc, opt.output_nc, opt.n_blocks).to(device)\n",
    "    net_G2 = Generator(opt.input_nc, opt.output_nc, opt.n_blocks).to(device)\n",
    "    net_G3 = Generator(opt.input_nc, opt.output_nc, opt.n_blocks).to(device)\n",
    "    # Load state dicts\n",
    "    net_G.load_state_dict(torch.load(os.path.join(opt.checkpoints_dir, opt.name, f'netG_A_{opt.which_epoch}.pth'), map_location=device, weights_only=True))\n",
    "    net_G2.load_state_dict(torch.load(os.path.join(opt.checkpoints_dir, \"opensketch_style\", f'netG_A_{opt.which_epoch}.pth'), map_location=device, weights_only=True))\n",
    "    net_G3.load_state_dict(torch.load(os.path.join(opt.checkpoints_dir, \"contour_style\", f'netG_A_{opt.which_epoch}.pth'), map_location=device, weights_only=True))\n",
    "    # print('loaded', os.path.join(opt.checkpoints_dir, opt.name, f'netG_A_{opt.which_epoch}.pth'))\n",
    "    # Set model's test mode\n",
    "    net_G.eval()\n",
    "    net_G2.eval()\n",
    "    net_G3.eval()\n",
    "    cartoonGAN_model.eval()\n",
    "    transforms_r = [transforms.Resize(int(opt.size), Image.BICUBIC),\n",
    "                    transforms.ToTensor()]\n",
    "    the_transformation = transforms.Compose(transforms_r)\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    running = 0\n",
    "    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_100)\n",
    "    parameters =  cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 0\n",
    "    new_w = 20\n",
    "    new_h = 20\n",
    "    full_w = 50\n",
    "    full_h = 50\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        cv2.startWindowThread()\n",
    "        try:\n",
    "            if ret == True:\n",
    "                locs_and_ids = []\n",
    "                image = frame.copy()\n",
    "                image = cv2.resize(image, (456,256))\n",
    "                corners, ids, rejectedCandidates = detector.detectMarkers(image)\n",
    "            \n",
    "                if len(corners) > 0:\n",
    "                    # flatten the ArUco IDs list\n",
    "                    ids = ids.flatten()\n",
    "                    # loop over the detected ArUCo corners\n",
    "                    for (markerCorner, markerID) in zip(corners, ids):\n",
    "                        corners = markerCorner.reshape((4, 2))\n",
    "                        (topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    "                        topRight = (int(topRight[0]), int(topRight[1]))\n",
    "                        bottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "                        bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "                        topLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "                        if markerID==1:\n",
    "                            temp_a_dist = dist_func(topLeft, topRight)\n",
    "                            a = (topLeft[0]-int(temp_a_dist[0]), topLeft[1]-int(temp_a_dist[1]))\n",
    "                        elif markerID==2:\n",
    "                            # temp_b_dist = int((topRight[0]-topLeft[0])/300.0*80)\n",
    "                            temp_b_dist = dist_func(topLeft, topRight)\n",
    "                            b = (topRight[0]+int(temp_b_dist[0]), topRight[1]-int(temp_b_dist[1]))\n",
    "                        elif markerID==3:\n",
    "                            # temp_c_dist = int((bottomRight[0]-bottomLeft[0])/300.0*80)\n",
    "                            temp_c_dist = dist_func(bottomLeft, bottomRight)\n",
    "                            c = (bottomLeft[0]-int(temp_c_dist[0]), bottomLeft[1]+int(temp_c_dist[1]))\n",
    "                        elif markerID==4:\n",
    "                            # temp_d_dist = int((bottomRight[0]-bottomLeft[0])/300.0*80)\n",
    "                            temp_d_dist = dist_func(bottomLeft, bottomRight)\n",
    "                            d = (bottomRight[0]+int(temp_d_dist[0]), bottomRight[1]+int(temp_d_dist[1]))\n",
    "                        cv2.line(image, topLeft, topRight, (0, 255, 0), 2)\n",
    "                        cv2.line(image, topRight, bottomRight, (0, 255, 0), 2)\n",
    "                        cv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)\n",
    "                        cv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)\n",
    "                if a != 0 and b != 0 and c != 0 and d != 0:\n",
    "                    src = np.float32([a,c,d,b])\n",
    "                else:\n",
    "                    src = np.float32([((full_w-new_w)/2, (full_h-new_h)/2), \n",
    "                                      ((full_w-new_w)/2, (full_h-new_h)/2+new_h), \n",
    "                                      ((full_w-new_w)/2+new_w,  (full_h-new_h)/2+new_h), \n",
    "                                      ((full_w-new_w)/2+new_w,  (full_h-new_h)/2)])\n",
    "\n",
    "                # half = cv2.resize(image, (1920, 1080))\n",
    "                half = image.copy()\n",
    "                frame_rgb = cv2.cvtColor(half, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                img_r = the_transformation(pil_image)\n",
    "                input_dict = {'r': img_r}\n",
    "                img_r2 = Variable(input_dict['r']).to(device)\n",
    "                real_A = img_r2\n",
    "\n",
    "                if switch_it==0:\n",
    "                    new_image = net_G(real_A)\n",
    "                    image_np = new_image.cpu().detach().numpy()  # Move to CPU and convert to NumPy\n",
    "                    image_np = np.transpose(image_np, (1, 2, 0))\n",
    "                    image_np = (image_np * 255).astype(np.uint8)\n",
    "                    image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                    the_output_image = image_bgr\n",
    "                elif switch_it==1:\n",
    "                    new_image2 = net_G2(real_A)\n",
    "                    image_np2 = new_image2.cpu().detach().numpy()\n",
    "                    image_np2 = np.transpose(image_np2, (1, 2, 0))\n",
    "                    image_np2 = (image_np2 * 255).astype(np.uint8)\n",
    "                    image_bgr2 = cv2.cvtColor(image_np2, cv2.COLOR_RGB2BGR)\n",
    "                    the_output_image = image_bgr2\n",
    "                elif switch_it==2:\n",
    "                    new_image3 = net_G3(real_A)\n",
    "                    image_np3 = new_image3.cpu().detach().numpy()\n",
    "                    image_np3 = np.transpose(image_np3, (1, 2, 0))\n",
    "                    image_np3 = (image_np3 * 255).astype(np.uint8)\n",
    "                    image_bgr3 = cv2.cvtColor(image_np3, cv2.COLOR_RGB2BGR)\n",
    "                    the_output_image = image_bgr3\n",
    "                elif switch_it==3:\n",
    "                    CG_input_image = pre_processing(pil_image)\n",
    "                    CG_transformed_image = cartoonGAN_model(CG_input_image)\n",
    "                    CG_output_image = post_processing(CG_transformed_image)\n",
    "                    CG_bgr_output = cv2.cvtColor(CG_output_image.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                    the_output_image = CG_bgr_output\n",
    "\n",
    "                # cv2.imshow('smaller', half)\n",
    "                # cv2.imshow('output', the_output_image)\n",
    "                masked = mask_picture_frame(image, src, the_output_image)\n",
    "                cv2.imshow('masked', masked)\n",
    "\n",
    "                pressedKey = cv2.waitKey(1) & 0xFF\n",
    "                if pressedKey == ord('q'):\n",
    "                    break\n",
    "                elif pressedKey == ord('s'):\n",
    "                    cv2.imwrite('full_frame_output.png', image_bgr)\n",
    "                elif pressedKey == ord('w'):\n",
    "                    switch_it+=1\n",
    "                    if switch_it==4:\n",
    "                        switch_it = 0\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    for i in range(5):\n",
    "        cv2.waitKey(1)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fa14f-8511-4127-9a02-1930703e35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_picture_frame(image, src, the_output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e46647-7660-4eb2-871d-7ef3d19cb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_output_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272bf80-9e00-4aa0-bb0b-0e7822ad2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ce481-9083-4b90-a839-f3c434584303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
